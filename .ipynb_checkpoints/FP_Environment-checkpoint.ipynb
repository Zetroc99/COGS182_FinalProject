{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project Checkpoint 2\n",
    "## Marlon Cortez\n",
    "## A14604525"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import draw\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Straight line track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.,  1.,  1.,  1.,  1., -1.],\n",
       "       [-1.,  0.,  0.,  0.,  0., -1.],\n",
       "       [-1.,  0.,  0.,  0.,  0., -1.],\n",
       "       [-1.,  0.,  0.,  0.,  0., -1.],\n",
       "       [-1.,  0.,  0.,  0.,  0., -1.],\n",
       "       [-1.,  0.,  0.,  0.,  0., -1.],\n",
       "       [-1.,  0.,  0.,  0.,  0., -1.],\n",
       "       [-1.,  0.,  0.,  0.,  0., -1.],\n",
       "       [-1.,  0.,  0.,  0.,  0., -1.],\n",
       "       [-1.,  0.,  0.,  0.,  0., -1.],\n",
       "       [-1.,  0.,  0.,  0.,  0., -1.],\n",
       "       [-1.,  0.,  0.,  0.,  0., -1.],\n",
       "       [-1.,  0.,  0.,  0.,  0., -1.],\n",
       "       [-1.,  0.,  0.,  0.,  0., -1.],\n",
       "       [-1.,  0.,  0.,  0.,  0., -1.],\n",
       "       [-1.,  0.,  0.,  0.,  0., -1.],\n",
       "       [-1.,  0.,  0.,  0.,  0., -1.],\n",
       "       [-1.,  0.,  0.,  0.,  0., -1.],\n",
       "       [-1.,  0.,  0.,  0.,  0., -1.],\n",
       "       [-1., -1., -1., -1., -1., -1.]])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpleTrack = np.zeros((20,6))\n",
    "simpleTrack[0] = np.ones(6)\n",
    "simpleTrack[19,1:-1] = -1\n",
    "simpleTrack[:,0] = -1\n",
    "simpleTrack[:,-1] = -1\n",
    "simpleTrack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single turn track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,  1.],\n",
       "       [-1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.],\n",
       "       [-1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.],\n",
       "       [-1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.],\n",
       "       [-1.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1., -1., -1.,  1.],\n",
       "       [-1.,  0.,  0.,  0., -1., -1., -1., -1.,  0.,  0.,  0.,  0.],\n",
       "       [-1.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [-1.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [-1.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [-1.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [-1.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [-1., -1., -1., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "turn = np.zeros((12, 12))\n",
    "rr, cc = draw.polygon_perimeter([11,11,5,4,0,0,0],\n",
    "                                [0,3,4,11,11,5,0],\n",
    "                            shape=turn.shape, clip=True)\n",
    "turn[rr, cc] = -1\n",
    "lastCol = turn[:,11]\n",
    "lastCol[lastCol == -1] = 1\n",
    "lastRow = turn[11:]\n",
    "lastRow[lastRow == -1] = -1\n",
    "turn[-1,0], turn[-1, 3] = -1,-1\n",
    "turnTrack = turn\n",
    "turnTrack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Actions: UP(0), RIGHT(1), DOWN(2), LEFT(3)\n",
    "State: (row, col)\n",
    "\"\"\"\n",
    "def nextMove(state, action, track):\n",
    "\n",
    "    if action == 0:\n",
    "        nextState = (state[0] - 1, state[1])\n",
    "    elif action == 1:\n",
    "        nextState = (state[0], state[1] + 1)\n",
    "    elif action == 2:\n",
    "        nextState = (state[0] + 1, state[1])\n",
    "    else:\n",
    "        nextState = (state[0], state[1] - 1)\n",
    "        \n",
    "    reward = track[nextState[0],nextState[1]]\n",
    "    return nextState, reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Environment on single-turn track with random actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting point is (11, 2) and Action selection is random \n",
      "\n",
      "State:  (11, 2)   Action:  0   Reward:  0.0\n",
      "State:  (10, 2)   Action:  0   Reward:  0.0\n",
      "State:  (9, 2)   Action:  1   Reward:  0.0\n",
      "State:  (9, 3)   Action:  1   Reward:  -1.0\n"
     ]
    }
   ],
   "source": [
    "track = turnTrack\n",
    "S  = (11, 2)\n",
    "A = 0\n",
    "R = 0.0\n",
    "\n",
    "print(\"Starting point is (11, 2) and Action selection is random \\n\")\n",
    "while True:\n",
    "    print(\"State: \", S, \"  Action: \", A, \"  Reward: \", R)\n",
    "    S, R = nextMove(S,A,track)\n",
    "    if R == -1 or R == 1:\n",
    "        print(\"State: \", S, \"  Action: \", A, \"  Reward: \", R)\n",
    "        break\n",
    "    A = np.random.choice([0,1,2,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Environment on straight track only going UP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting point is (19, 2) only going up \n",
      "\n",
      "State:  (19, 2)   Action:  0   Reward:  0.0\n",
      "State:  (18, 2)   Action:  0   Reward:  0.0\n",
      "State:  (17, 2)   Action:  0   Reward:  0.0\n",
      "State:  (16, 2)   Action:  0   Reward:  0.0\n",
      "State:  (15, 2)   Action:  0   Reward:  0.0\n",
      "State:  (14, 2)   Action:  0   Reward:  0.0\n",
      "State:  (13, 2)   Action:  0   Reward:  0.0\n",
      "State:  (12, 2)   Action:  0   Reward:  0.0\n",
      "State:  (11, 2)   Action:  0   Reward:  0.0\n",
      "State:  (10, 2)   Action:  0   Reward:  0.0\n",
      "State:  (9, 2)   Action:  0   Reward:  0.0\n",
      "State:  (8, 2)   Action:  0   Reward:  0.0\n",
      "State:  (7, 2)   Action:  0   Reward:  0.0\n",
      "State:  (6, 2)   Action:  0   Reward:  0.0\n",
      "State:  (5, 2)   Action:  0   Reward:  0.0\n",
      "State:  (4, 2)   Action:  0   Reward:  0.0\n",
      "State:  (3, 2)   Action:  0   Reward:  0.0\n",
      "State:  (2, 2)   Action:  0   Reward:  0.0\n",
      "State:  (1, 2)   Action:  0   Reward:  1.0\n"
     ]
    }
   ],
   "source": [
    "track = simpleTrack\n",
    "S  = (19, 2)\n",
    "A = 0\n",
    "R = 0.0\n",
    "\n",
    "print(\"Starting point is (19, 2) only going up \\n\")\n",
    "while True:\n",
    "    nS, R = nextMove(S,A,track)\n",
    "    print(\"State: \", S, \"  Action: \", A, \"  Reward: \", R)\n",
    "    if R == -1 or R == 1:\n",
    "        break\n",
    "    S = nS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo Control ES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MC_ES_ep(policy, Q, start_state, track, count):\n",
    "    \n",
    "    state = start_state\n",
    "    saR = []\n",
    "    \n",
    "    while True:\n",
    "        action = np.random.choice([0,1,2,3])\n",
    "\n",
    "        if state in count:\n",
    "            count[state][action] += 1\n",
    "        else:\n",
    "            count[state] = [1 if i == action else 0 for i in range(4)]\n",
    "                   \n",
    "        newState, r = nextMove(state, action, track)\n",
    "        saR.append([state, action, r])\n",
    "        \n",
    "        if r != 0:\n",
    "            break\n",
    "        state = newState\n",
    "        \n",
    "    saR.reverse()\n",
    "    G = 0\n",
    "    for t in saR:\n",
    "        action = t[1]\n",
    "        state = t[0]\n",
    "        G = t[2] + G\n",
    "        if state in count:\n",
    "            a = (1.0/ count[state][action])\n",
    "            Q[state][action] += a*(G - Q[state][action])\n",
    "            policy[state[0], state[1]] = np.argmax(Q[state])\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement Monte Carlo ES on simpleTrack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 3., 0.],\n",
       "       [0., 2., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 2., 0., 0., 0.],\n",
       "       [0., 0., 0., 3., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "track = simpleTrack\n",
    "r,c = np.random.choice(np.arange(1, 19)), np.random.choice(np.arange(1, 6))\n",
    "start_state = r,c\n",
    "                                                         \n",
    "num_states = track.shape[0]*track.shape[1]\n",
    "\n",
    "policyS = np.zeros([track.shape[0], track.shape[1]])\n",
    "#Q = np.zeros([len(track[:]), len(track[0]), 4])\n",
    "Q = defaultdict(lambda: np.zeros(4))\n",
    "\n",
    "count = {}\n",
    "\n",
    "eps = 100000\n",
    "\n",
    "for j in range(eps):     \n",
    "    policyS = MC_ES_ep(policyS, Q, start_state, track, count)\n",
    "    \n",
    "policyS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement Monte Carlo ES on turnTrack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 2., 1., 2., 1., 1., 1., 1., 1., 1., 1., 0.],\n",
       "       [0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.],\n",
       "       [0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.],\n",
       "       [0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 3., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "track = turnTrack\n",
    "r,c = np.random.choice(np.arange(1, 5)), np.random.choice(np.arange(1, 8))\n",
    "start_state = r,c\n",
    "                                                         \n",
    "num_states = track.shape[0]*track.shape[1]\n",
    "policyT = np.zeros([track.shape[0], track.shape[1]])\n",
    "#Q = np.zeros([len(track[:]), len(track[0]), 4])\n",
    "Q = defaultdict(lambda: np.zeros(4))\n",
    "\n",
    "count  = {}\n",
    "\n",
    "eps = 100000\n",
    "\n",
    "for j in range(eps):     \n",
    "    policyT = MC_ES_ep(policyT, Q, start_state, track, count)\n",
    "    \n",
    "policyT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy(e, state, policy):\n",
    "    be_greedy = np.random.random() < e\n",
    "    if be_greedy:\n",
    "        action = policy[state]\n",
    "    else:\n",
    "        action = np.random.randint(0,4)\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TD Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TD_lmbda_ep(policy, track, start_state, values, lmbda, alpha, gamma):\n",
    "    \n",
    "    e_weights = np.zeros([len(track[:]), len(track[0])])\n",
    "    state = start_state\n",
    "   \n",
    "    for t in range(10000):\n",
    "        action = epsilon_greedy(0.1, state, policy)\n",
    "        state_new, r = nextMove(state, action, track)\n",
    "        \n",
    "        td_error = r + gamma*values[state_new[0], state_new[1]] - values[state[0], state[1]]\n",
    "        e_weights[state[0], state[1]] = 1\n",
    "    \n",
    "        #update values and eligibility weights\n",
    "        values = values + alpha*td_error*e_weights\n",
    "        e_weights = gamma*lmbda*e_weights\n",
    "        state = state_new\n",
    "        \n",
    "        if r != 0:\n",
    "            break\n",
    "    return values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement TD Lamda for simpleTrack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.18,  0.41,  0.38, -0.05,  0.  ],\n",
       "       [ 0.  , -0.34,  0.02,  0.02, -0.17,  0.  ],\n",
       "       [ 0.  , -0.47, -0.36, -0.48, -0.47,  0.  ],\n",
       "       [ 0.  , -0.89, -0.64, -0.52, -0.71,  0.  ],\n",
       "       [ 0.  , -0.92, -0.84, -0.56, -0.68,  0.  ],\n",
       "       [ 0.  , -0.96, -0.96, -0.8 , -0.92,  0.  ],\n",
       "       [ 0.  , -0.97, -0.98, -0.93, -0.95,  0.  ],\n",
       "       [ 0.  , -0.98, -0.95, -0.95, -0.94,  0.  ],\n",
       "       [ 0.  , -0.96, -0.98, -0.99, -0.97,  0.  ],\n",
       "       [ 0.  , -0.98, -1.  , -0.99, -0.99,  0.  ],\n",
       "       [ 0.  , -0.99, -0.99, -1.  , -1.  ,  0.  ],\n",
       "       [ 0.  , -1.  , -1.  , -1.  , -1.  ,  0.  ],\n",
       "       [ 0.  , -1.  , -1.  , -1.  , -1.  ,  0.  ],\n",
       "       [ 0.  , -1.  , -1.  , -1.  , -1.  ,  0.  ],\n",
       "       [ 0.  , -1.  , -1.  , -1.  , -1.  ,  0.  ],\n",
       "       [ 0.  , -1.  , -1.  , -1.  , -1.  ,  0.  ],\n",
       "       [ 0.  , -1.  , -1.  , -1.  , -1.  ,  0.  ],\n",
       "       [ 0.  , -1.  , -1.  , -1.  , -1.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ]])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha = 0.1\n",
    "lmbda = 1.0\n",
    "gamma = 1.0\n",
    "\n",
    "track = simpleTrack\n",
    "start_state = (18, np.random.choice([1,2,3,4]))\n",
    "num_states = track.shape[0]*track.shape[1]\n",
    "\n",
    "eps = 100000\n",
    "\n",
    "values = np.zeros((len(track[:]), len(track[0])))\n",
    "\n",
    "for j in range(eps):   \n",
    "    values = TD_lmbda_ep(policyS, track, start_state, values, lmbda, alpha, gamma)\n",
    "    \n",
    "np.around(values,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement TD lambda turnTrack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,\n",
       "         0. ],\n",
       "       [ 0. , -0.5, -0.5, -0.6, -0.5, -0.5, -0.5, -0.4, -0.2,  0. ,  0.1,\n",
       "         0. ],\n",
       "       [ 0. , -0.6, -0.5, -0.4, -0.4, -0.3, -0.2, -0.2, -0.1,  0. ,  0.2,\n",
       "         0. ],\n",
       "       [ 0. , -0.8, -0.7, -0.5, -0.5, -0.3, -0.3, -0.3, -0.2, -0.1,  0.1,\n",
       "         0. ],\n",
       "       [ 0. , -0.9, -0.8, -0.7, -0.7, -0.4, -0.4, -0.5,  0. ,  0. ,  0. ,\n",
       "         0. ],\n",
       "       [ 0. , -0.9, -0.9, -0.9,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,\n",
       "         0. ],\n",
       "       [ 0. , -1. , -0.9, -1. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,\n",
       "         0. ],\n",
       "       [ 0. , -1. , -1. , -1. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,\n",
       "         0. ],\n",
       "       [ 0. , -1. , -1. , -1. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,\n",
       "         0. ],\n",
       "       [ 0. , -1. , -1. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,\n",
       "         0. ],\n",
       "       [ 0. , -1. , -1. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,\n",
       "         0. ],\n",
       "       [ 0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,\n",
       "         0. ]])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha = 0.1\n",
    "lmbda = 0.5\n",
    "gamma = 1.0\n",
    "\n",
    "track = turnTrack\n",
    "start_state = (10, np.random.choice([1,2]))\n",
    "num_states = track.shape[0]*track.shape[1]\n",
    "\n",
    "eps = 10000\n",
    "values = np.zeros((len(track[:]), len(track[0])))\n",
    "    \n",
    "for j in range(eps):\n",
    "    values = TD_lmbda_ep(policyT, track, start_state, values, lmbda, alpha, gamma)\n",
    "        \n",
    "np.around(values, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q- Learn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
